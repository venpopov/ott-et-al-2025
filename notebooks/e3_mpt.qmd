---
title: "Reproduce MPT analysis of E3"
format: html
author: Ven Popov
execute: 
  error: true
---

```{r setup}
#| include: false
knitr::opts_knit$set(root.dir = here::here())
options(tidyverse.quiet = TRUE, readr.show_col_types = FALSE, mc.cores = 4, brms.backend = "cmdstanr")
```

```{r}
#| label: package-load
#| message: false
library(readr)
library(dplyr)
library(tidyr)
library(knitr)
library(MPTinR)
library(brms)
library(bmm)
library(cmdstanr)
```

Get the number of observations per condition and response type for the source
```{r}
#| label: load-data
events_file <- "data/E3_events.csv"

if (!file.exists(events_file)) {
  source("R/export_aggregate_events.r")
  read_csv("data/E3_data.csv") |> 
    aggregate_event_counts() |> 
    write_csv(events_file)
} 

data <- read_csv(events_file)
```

It is in long format:

```{r}
#| label: preview-data
head(data, n = 20) |> kable()
```


## Fit authors' MPT model with MPTinR

The full model in the paper is: (figure from the paper):

![](./mpt_tree_figure.png)

which we can express as

```{r}
#| label: full_model
full_model <- "
# Responses to R Items from Source A (order: Source A, Source B and New)
D_R * (1-d_RA) * a + (1-D_R) * b * g + D_R * d_RA
D_R * (1-d_RA) * (1-a) + (1-D_R) * b * (1-g)
(1-D_R) * (1-b)

# Responses to R Items from Source B (order: Source A, Source B and New)
D_R * (1-d_RB) * a + (1-D_R) * b * g
D_R * (1-d_RB) * (1-a) + (1-D_R) * b * (1-g) + D_R * d_RB
(1-D_R) * (1-b)

# Responses to F Items from Source A (order: Source A, Source B and New)
D_F * (1-d_FA) * a + (1-D_F) * b * g + D_F * d_FA
D_F * (1-d_FA) * (1-a) + (1-D_F) * b * (1-g)
(1-D_F) * (1-b)

# Responses to F Items from Source B (order: Source A, Source B and New)
D_F * (1-d_FB) * a + (1-D_F) * b * g
D_F * (1-d_FB) * (1-a) + (1-D_F) * b * (1-g) + D_F * d_FB
(1-D_F) * (1-b)

# Responses to New Items (order: Source A, Source B and New)
(1-D_N) * b * g
(1-D_N) * b * (1-g)
(1-D_N) * (1-b) + D_N
"
```

Check that the model specification doesn't violate any assumptions:

```{r}
#| label: check_model
check.mpt(textConnection(full_model))
```

They restricted some parameters: 

"We set $d_{RA}$ = $d_{RB}$ (labeled as $d_R$), $d_{FA}$ = $d_{FB}$ (labeled as $d_{F}$), and $D_{new}$ = $D_{R}$"

which we can specify as a string as well:

```{r}
#| label: mpt_restriction
mpt_restrictions <- "
d_RA = d_RB
d_FA = d_FB
D_R = D_N
"
```

For MPTinR we need the data to be a matrix where each row is an entire dataset (e.g. participant), and the columns represent the number of responses for each condition by response combination. The order needs to match the order of the model components listed above. We'll need to do some data magic. First ensure correct ordering:

```{r}
#| label: rearrange-data
data <- data |> 
  mutate(item_type = case_when(
    item_type == "R item" ~ "R",
    item_type == "F item" ~ "F",
    item_type == "distractor" ~ "D"
  )) |> 
  arrange(id, desc(item_type), source, resp)
data[data$item_type == "D",]$item_type <- "N"
head(data, 15)
```

then pivot to wide and change to matrix:

```{r}
#| label: df-to-wide-matrix
data_mat <- data |> 
  pivot_wider(names_from = c("item_type", "source", "resp"), values_from = "n") |> 
  select(-id) |> 
  as.matrix()
str(data_mat)
colnames(data_mat)
```

and now we can finally estimate the model with MPTinR:

```{r}
#| label: mpt_fit
#| cache: true
#| collapse: true
#| comment: "#>"
mpt_fit <- fit.mpt(
  data = data_mat, 
  model.filename = textConnection(full_model), 
  restrictions.filename = textConnection(mpt_restrictions)
)
```

We get lots of convergence warnings for the individual subjects. Not surprising since many participants lack observations in some cells. 

The authors of the paper only fitted the model on the aggregated data. `fit.mpt` automatically fits both the individual subjects and the aggregated data, so we can check out the aggregate fits. First, some info statistics:

```{r}
#| label: mpt-statistics
mpt_fit$goodness.of.fit$aggregated
mpt_fit$information.criteria$aggregated
mpt_fit$model.info$aggregated
```

And the parameter estimates:

```{r}
#| label: mpt-aggregated-parameter-estimates
agg_est <- mpt_fit$parameters$aggregated
par_order <- c("a", "b", "g", "D_R", "D_F", "d_RA", "d_FA")
idx <- match(par_order, rownames(agg_est))
print(agg_est[idx,], digits = 3)
```

Thankfully that matches exactly the results from the paper, so I know I didn't mess up anything in the data transformation.

## Fit authors' MPT model with BRMS: Attemp 1

As I documented in a [GitHub issue](https://github.com/venpopov/bmm/issues/255) for `bmm`, it is possible to estimate MPT models in brms with a multinomial family and some clever hacks of the brms formula syntax. In the linked issue, I did it only for a simple two-high-threshold model of an old/new recognition task. Let's see if I can find a way to use the same approach here.

For the brms approach, we need the data in long-format, but with each response-type in a separate column. 

```{r}
#| label: data_brms
(data_brms <- data |> 
  pivot_wider(names_from = c("resp"), values_from = c("n")) |> 
  mutate(total = A + B + N))
```

We need dummy variables for each of the five trees:

```{r}
(data_brms <- data_brms |> 
  mutate(
    tree1 = as.numeric(item_type == "R" & source == "A"),
    tree2 = as.numeric(item_type == "R" & source == "B"),
    tree3 = as.numeric(item_type == "F" & source == "A"),
    tree4 = as.numeric(item_type == "F" & source == "B"),
    tree5 = as.numeric(item_type == "N")
  ))
```

With the help of the dummy variables we can construct one formula for each of the three response types. We use these dummy variables to "turn on/off" different parts of the formula.

There are many different ways to construct the formulas. In this one we skip the parameter indices, and we will let parameters vary by condition. When we do that we observe a lot of duplication we could reduce, but let's keep it explicit for now.

```{r}
#| label: mpt_base_formula
# package the responses as a matrix
data_brms$y <- with(data_brms, cbind(A, B, N))
mpt_base_formula <- bf(y | trials(total) ~ 1, nl = TRUE)
mpt_mus_formulas <- list(
  nlf(
    muA ~ logit(
      tree1 * (D*(1-d)*a + (1-D)*b*g + D*d) + 
      tree2 * (D*(1-d)*a + (1-D)*b*g) + 
      tree3 * (D*(1-d)*a + (1-D)*b*g + D*d) + 
      tree4 * (D*(1-d)*a + (1-D)*b*g) + 
      tree5 * ((1-D)*b*g)
    )
  ),
  nlf(
    muB ~ logit(
      tree1 * (D*(1-d)*(1-a) + (1-D)*b*(1-g)) +
      tree2 * (D*(1-d)*(1-a) + (1-D)*b*(1-g) + D*d) +
      tree3 * (D*(1-d)*(1-a) + (1-D)*b*(1-g)) +
      tree4 * (D*(1-d)*(1-a) + (1-D)*b*(1-g) + D*d) +
      tree5 * ((1-D)*b*(1-g))
    )
  ),
  nlf(
    muN ~ logit(
      tree1 * ((1-D)*(1-b)) +
      tree2 * ((1-D)*(1-b)) +
      tree3 * ((1-D)*(1-b)) +
      tree4 * ((1-D)*(1-b)) +
      tree5 * ((1-D)*(1-b) + D)
    )
  )
) 
```

We wrap each formula in logit(), because the multinomial family expect an unbound predictor. The parameters should also be sample on a logit scale, so we need to add some transformations:

```{r}
#| label: mpt_par_formulas
mpt_par_formulas <- list(
  nlf(D ~ inv_logit(lD)),
  nlf(d ~ inv_logit(ld)),
  nlf(a ~ inv_logit(la)),
  nlf(b ~ inv_logit(lb)),
  nlf(g ~ inv_logit(lg))
)
```

and finally our actual predictor formulas. We let D vary by item_type (although the source made D_R == D_N, we can't do it with this coding - will do later). First, ignore random effects for a sanity check:

```{r}
#| label: mpt_pred_formulas
mpt_pred_formulas <- list(
  lf(lD ~ 0 + item_type),
  lf(ld ~ 0 + item_type),
  lf(la ~ 1),
  lf(lb ~ 1),
  lf(lg ~ 1)
)
```

I immediately notice a problem. We want ld to vary by item_type, but that parameter doesn't matter for new items, so sampling it will not influence the likelihood. That might cause sampling problems. One solution is to try to fix it via a prior. Tricky... the other is to make the parameters again explicit. Let's see how it goes as it is.

We combine all the formulas together:

```{r}
#| label: mpt_form1
(mpt_form1 <- Reduce(`+`, c(mpt_mus_formulas, mpt_par_formulas, mpt_pred_formulas), init = mpt_base_formula))
```

We put logistic priors on all the relevant parameters (uniform on the probability scale):

```{r}
#| label: mpt_prior1
mpt_prior1 <- 
  prior("logistic(0, 1)", nlpar = "lD", class = "b") +
  prior("logistic(0, 1)", nlpar = "ld", class = "b") +
  prior("logistic(0, 1)", nlpar = "la", class = "b") +
  prior("logistic(0, 1)", nlpar = "lb", class = "b") +
  prior("logistic(0, 1)", nlpar = "lg", class = "b")
```

and now we are ready to try fitting this:

```{r}
#| label: fit_mpt_brm1
#| cache: true
fit_mpt_brm1 <- brm(
  mpt_form1, 
  data = data_brms, 
  family = multinomial(refcat = NA),
  prior = mpt_prior1
)
```

No scary convergence messages, though Rhat is marginal (should increase samples):

```{r}
#| label: summary-fit_mpt_brm1
summary(fit_mpt_brm1)
```

Already can notice teh huge range for ld_item_typeN, which makes sense because as I noted that condition does not have an effect on the likelihood. Let's turn into proportion scale to compare with original model:

```{r}
#| label: summary-fit_mpt_brm1_invlogit
inv_logit <- function(x) 1/(1+exp(-x))
fit_mpt_brm1 |> 
  as_draws_array(variable = "b_l._", regex = TRUE) |> 
  inv_logit() |> 
  summary()
```

a, b and g are roughly the same as the non-brms model. The others are not so much. D_F = 0.246, D_n = 0.296, D_R = 0.598. In the original model D_R was constrained to be the same as D_N, which here it's clearly not. But is that a problem with the brms model or would it also appear if we did not constrain D_R = D_N in the non-brms model? I'll come back to this. Let's look at some plots:

```{r}
#| label: plot-fit_mpt_brm1
plot(fit_mpt_brm1, ask = FALSE)
```

Some strange posterior distributions. How about the pairs plot?

```{r}
#| label: pairs-plot-fit_mpt_brm1
pairs(fit_mpt_brm1)
```

alright that looks bad! Clear screaming issues with identifiability. 

As a curiousity, how does the non-bayesian model fare if we relax the D_N = D_R constraint:

```{r}
#| label: mpt_fit2
mpt_fit2 <- fit.mpt(
  data = colSums(data_mat), 
  model.filename = textConnection(full_model), 
  restrictions.filename = textConnection("d_RA = d_RB\nd_FA = d_FB")
)
mpt_fit2$goodness.of.fit
print(mpt_fit2$parameters, digits = 3)
```

## Fit authors' MPT model with BRMS: Attemp 2

Small change first. Fix the ld_item_typeN parameter to be a constant 0

```{r}
#| label: fit_mpt_brm2
#| cache: true
mpt_prior2 <- mpt_prior1 +
  prior("constant(0)", nlpar = "ld", class = "b", coef = "item_typeN")
fit_mpt_brm2 <- brm(
  mpt_form1, 
  data = data_brms, 
  family = multinomial(refcat = NA),
  prior = mpt_prior2
)
```

summary:

```{r}
#| label: ssummary-fit_mpt_brm2
summary(fit_mpt_brm#| 2)
fit_mpt_brm2 |> 
  as_draws_array(variable = "b_l._", regex = TRUE) |> 
  inv_logit() |> 
  summary()
```

This didn't really affect any of the other estimates:

```{r}
#| label: pairs-fit_mpt_brm2
pairs(fit_mpt_brm2)
```

## Fit authors' MPT model with BRMS: Attemp 3

Let's try fixing D_R = D_N. Two approaches:

1. Create a new variable item_type2 which codes R and N items the same
2. Build it into the formula


```{r}
#| label: fit_mpt_brm3
#| cache: true
data_brms <- data_brms  |> 
  mutate(D_type = if_else(item_type == "F", "F", "RN"))
mpt_form3 <- mpt_form1 + lf(lD ~ 0 + D_type)

fit_mpt_brm3 <- brm(
  mpt_form3, 
  data = data_brms, 
  family = multinomial(refcat = NA),
  prior = mpt_prior2
)
```

First off, the estimation was much faster, which is a good sign. Bulk_ESS are much higher also. The d_F parameter is suspiciously variable.


```{r}
#| label: summary-fit_mpt_brm3
summary(fit_mpt_brm3)
```

parameter estimates on the proportion scale:

```{r}
#| label: summary-invlogit-fit_mpt_brm3
fit_mpt_brm3 |> 
  as_draws_array(variable = "^b_l._", regex = TRUE) |> 
  inv_logit() |> 
  summary()
```

the problem is that these are not that similar to the non-bayesian estimates

b is about 0.495 rather than 0.35, D_R/N is 0.471 not 0.688, and although d_F > d_R, and they are "in the ballpark", the difference from 0.879 and 0.395 is still substantial.

I suspect there is still parameter trade-offs. Let's check the pairs:

```{r}
#| label: pairs-fit_mpt_brm3
# pdf("output/fit_mpt_brm3_pairs.pdf", width = 11, height = 11)
pairs(fit_mpt_brm3)
# dev.off()
```

some minor issues, but nothing like what we had before. Everything seems to have mixed well also (see regular plot). It is possible that the model I have specified differs.

Maaaybe the problem is the logit wrapper around the mu predictors? The more I think about it the more I'm sure. 

brms applies `softmax` on the mu vector. assume the raw values for mu to be (without my logit wrapper) `mu_prob` (3 values). 

Then currently we have

```
y ~ multinomial(softmax(logit(mu_prob)))
```

with 

```
logit(mu_prob) = log(mu_prop) - log(1 - mu_prop)
softmax(logit(mu_prob)) = exp(logit(mu_prob)) / sum(exp(logit(mu_prob)))
softmax(logit(mu_prob)) = mu_prob/(1-mu_prob) / sum(mu_prob/(1-mu_prob))
```

yup, this is wrong. I should just take the log instead of the logit

```{r}
#| label: fit_mpt_brm4
#| cache: true
mpt_base_formula <- bf(y | trials(total) ~ 1, nl = TRUE)
mpt_mus_formulas4 <- list(
  nlf(
    muA ~ log(
      tree1 * (D*(1-d)*a + (1-D)*b*g + D*d) + 
      tree2 * (D*(1-d)*a + (1-D)*b*g) + 
      tree3 * (D*(1-d)*a + (1-D)*b*g + D*d) + 
      tree4 * (D*(1-d)*a + (1-D)*b*g) + 
      tree5 * ((1-D)*b*g)
    )
  ),
  nlf(
    muB ~ log(
      tree1 * (D*(1-d)*(1-a) + (1-D)*b*(1-g)) +
      tree2 * (D*(1-d)*(1-a) + (1-D)*b*(1-g) + D*d) +
      tree3 * (D*(1-d)*(1-a) + (1-D)*b*(1-g)) +
      tree4 * (D*(1-d)*(1-a) + (1-D)*b*(1-g) + D*d) +
      tree5 * ((1-D)*b*(1-g))
    )
  ),
  nlf(
    muN ~ log(
      tree1 * ((1-D)*(1-b)) +
      tree2 * ((1-D)*(1-b)) +
      tree3 * ((1-D)*(1-b)) +
      tree4 * ((1-D)*(1-b)) +
      tree5 * ((1-D)*(1-b) + D)
    )
  )
) 

mpt_par_formulas <- list(
  nlf(D ~ inv_logit(lD)),
  nlf(d ~ inv_logit(ld)),
  nlf(a ~ inv_logit(la)),
  nlf(b ~ inv_logit(lb)),
  nlf(g ~ inv_logit(lg))
)

mpt_pred_formulas4 <- list(
  lf(lD ~ 0 + D_type),
  lf(ld ~ 0 + item_type),
  lf(la ~ 1),
  lf(lb ~ 1),
  lf(lg ~ 1)
)


mpt_form4 <- Reduce(`+`, c(mpt_mus_formulas4, mpt_par_formulas, mpt_pred_formulas4), init = mpt_base_formula)

fit_mpt_brm4 <- brm(
  mpt_form4, 
  data = data_brms, 
  family = multinomial(refcat = NA),
  prior = mpt_prior2
)
```



```{r}
#| label: summary-fit_mpt_brm4
fit_mpt_brm4
```


```{r}
#| label: summary-invlogit-fit_mpt_brm4
fit_mpt_brm4 |> 
  as_draws_array(variable = "^b_l._", regex = TRUE) |> 
  inv_logit() |> 
  summary()
```

Now we are talking! These results are virtually identical to the non-brms estimates:

```{r}
#| label: original-agg-est
print(agg_est[idx,], digits = 3)
```

Now we can incorporate random effects:

```{r}
#| label: fit_mpt_brm5
#| cache: true

mpt_form5 <- mpt_form4 + lf(
  lD ~ 0 + D_type + (0 + D_type | id),
  ld ~ 0 + item_type + (0 + item_type | id),
  la ~ 1 + (1|id),
  lb ~ 1 + (1|id),
  lg ~ 1 + (1|id)
)

mpt_prior5 <- mpt_prior2 + prior("constant(0.00001)", class = "sd", nlpar = "ld", coef = "item_typeN", group = "id")


fit_mpt_brm5 <- brm(
  mpt_form5, 
  data = data_brms, 
  family = multinomial(refcat = NA),
  prior = mpt_prior5
)
```



```{r}
#| label: summary-invlogit-fit_mpt_brm5
summary(fit_mpt_brm5)
fit_mpt_brm5 |> 
  as_draws_array(variable = "^b_l._", regex = TRUE) |> 
  inv_logit() |> 
  summary()
```

There is substantial variability in the D_F, D_R, b, d_F and d_R parameters


```{r}
pred5 <- predict(fit_mpt_brm5)
pred5[,1,]

pred4 <- predict(fit_mpt_brm4)
pred4[,1,]

sqrt(mean((data_brms$y-pred4[,1,])**2))
sqrt(mean((data_brms$y-pred5[,1,])**2))

# G^2
2 * sum(data_brms$y*(log(data_brms$y)-log(pred5[,1,])), na.rm = T)
2 * sum(data_brms$y*(log(data_brms$y)-log(pred4[,1,])), na.rm = T)
```


## Fit the M3 model


```{r}
library(bmm)

bmmformula <- bmf(
  A ~ tree1*cR + tree3*cF + b + b2 + DRN*(tree1+tree2) + DF*(tree3+tree4) + a,
  B ~ tree2*cR + tree4*cF + b + b2 + DRN*(tree1+tree2) + DF*(tree3+tree4),
  N ~ b + DRN*tree5,
  cR ~ 1,
  cF ~ 1,
  b2 ~ 1,
  DRN ~ 1,
  DF ~ 1,
  a ~ 1
)

bmmodel <- m3(
  resp_cats = c("A", "B", "N"),
  num_options = c(1, 1, 1), 
  choice_rule = "softmax", 
  version = "custom",
  links = list(cR = "identity", cF = "identity", b2 = "identity", DRN = "identity", DF = "identity", a = "identity")
)

fitm3 <- bmm(
  formula = bmmformula,
  data = data_brms,
  model = bmmodel
)

predm3 <- predict(fitm3)
predm3[,1,]  |> head(10)
head(data_brms$y, 10)
sqrt(mean((predm3[,1,]-data_brms$y)**2))
sqrt(mean((pred4[,1,]-data_brms$y)**2))
```


with ranefs

```{r}
library(bmm)

bmmformula2 <- bmf(
  A ~ tree1*cR + tree3*cF + b + b2 + DRN*(tree1+tree2) + DF*(tree3+tree4) + a,
  B ~ tree2*cR + tree4*cF + b + b2 + DRN*(tree1+tree2) + DF*(tree3+tree4),
  N ~ b + DRN*tree5,
  cR ~ 1 + (1|id),
  cF ~ 1 + (1|id),
  b2 ~ 1 + (1|id),
  DRN ~ 1 + (1|id),
  DF ~ 1 + (1|id),
  a ~ 1 + (1|id)
)

bmmodel <- m3(
  resp_cats = c("A", "B", "N"),
  num_options = c(1, 1, 1), 
  choice_rule = "softmax", 
  version = "custom",
  links = list(cR = "identity", cF = "identity", b2 = "identity", DRN = "identity", DF = "identity", a = "identity")
)

fitm3_2 <- bmm(
  formula = bmmformula2,
  data = data_brms,
  model = bmmodel
)

predm3_2 <- predict(fitm3_2)
predm3_2[,1,]  |> head(10)
head(data_brms$y, 10)
sqrt(mean((predm3_2[,1,]-data_brms$y)**2))
sqrt(mean((pred5[,1,]-data_brms$y)**2))
```
